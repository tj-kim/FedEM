{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758e5088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/celeba/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *\n",
    "\n",
    "from transfer_attacks.TA_utils import *\n",
    "from transfer_attacks.Boundary_Transferer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d88e3f7",
   "metadata": {},
   "source": [
    "## Load Blackbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a83b6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 165.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 4.613 | Train Acc: 0.958% |Test Loss: 4.612 | Test Acc: 1.167% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"cifar100\"\n",
    "args_.method = \"FedAvg\"\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= 1\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar100/FedAvgBlackBox/'\n",
    "args_.validation = False\n",
    "args_.tune_steps = None\n",
    "\n",
    "# Generate the dummy values here\n",
    "aggregator, clients = dummy_aggregator(args_, num_user=40)\n",
    "\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "aggregator.load_state(args_.save_path)\n",
    "\n",
    "model_weights = []\n",
    "weights = np.load(args_.save_path + \"train_client_weights.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27146376",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 1\n",
    "# This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "# obtain the state dict for each of the weights \n",
    "weights_h = []\n",
    "\n",
    "for h in hypotheses:\n",
    "    weights_h += [h.model.state_dict()]\n",
    "\n",
    "for i in range(num_models):\n",
    "    model_weights += [weights[i]]\n",
    "\n",
    "# Generate the weights to test on as linear combinations of the model_weights\n",
    "models_test = []\n",
    "\n",
    "for (w0) in model_weights:\n",
    "    # first make the model with empty weights\n",
    "    new_model = copy.deepcopy(hypotheses[0].model)\n",
    "    new_model.eval()\n",
    "    new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "    for key in weights_h[0]:\n",
    "        new_weight_dict[key] = w0[0]*weights_h[0][key] \n",
    "    new_model.load_state_dict(new_weight_dict)\n",
    "    models_test += [new_model]\n",
    "    \n",
    "adv_model = copy.deepcopy(models_test[0])\n",
    "    \n",
    "del aggregator, clients, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38462229",
   "metadata": {},
   "source": [
    "## Load Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab806de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 501.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:34<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Test Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 4.608 | Train Acc: 0.812% |Test Loss: 4.608 | Test Acc: 0.958% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "setting = 'FedEM'\n",
    "\n",
    "if setting == 'FedEM':\n",
    "    n = 3\n",
    "else:\n",
    "    n = 1\n",
    "\n",
    "\n",
    "# Manually set argument parameters\n",
    "args_ = Args()\n",
    "args_.experiment = \"cifar100\"\n",
    "args_.method = setting\n",
    "args_.decentralized = False\n",
    "args_.sampling_rate = 1.0\n",
    "args_.input_dimension = None\n",
    "args_.output_dimension = None\n",
    "args_.n_learners= n\n",
    "args_.n_rounds = 10\n",
    "args_.bz = 128\n",
    "args_.local_steps = 1\n",
    "args_.lr_lambda = 0\n",
    "args_.lr =0.03\n",
    "args_.lr_scheduler = 'multi_step'\n",
    "args_.log_freq = 10\n",
    "args_.device = 'cuda'\n",
    "args_.optimizer = 'sgd'\n",
    "args_.mu = 0\n",
    "args_.communication_probability = 0.1\n",
    "args_.q = 1\n",
    "args_.locally_tune_clients = False\n",
    "args_.seed = 1234\n",
    "args_.verbose = 1\n",
    "args_.save_path = 'weights/cifar100/fedem/'\n",
    "args_.validation = False\n",
    "args_.tune_steps = None\n",
    "\n",
    "# Generate the dummy values here\n",
    "aggregator, clients = dummy_aggregator(args_, num_user=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebb78c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Validation Data across all clients as test\n",
    "data_x = []\n",
    "data_y = []\n",
    "\n",
    "for i in range(len(clients)):\n",
    "    daniloader = clients[i].val_iterator\n",
    "    for (x,y,idx) in daniloader.dataset:\n",
    "        data_x.append(x)\n",
    "        data_y.append(y)\n",
    "\n",
    "data_x = torch.stack(data_x)\n",
    "data_y = torch.tensor(data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1f9c8",
   "metadata": {},
   "source": [
    "## Load Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b445be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 40\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "\n",
    "if setting == 'local':\n",
    "\n",
    "#     args_.save_path = 'weights/final/femnist/fig1_take3/local_benign/'\n",
    "#     args_.save_path ='weights/final/femnist/fig1_take3/local_adv/'\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    model_weights = []\n",
    "#     weights = np.load(\"weights/final/femnist/fig1_take3/local_benign/train_client_weights.npy\")\n",
    "    weights = np.load(args_.save_path+'train_client_weights.npy')\n",
    "    \n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        new_model = copy.deepcopy(aggregator.clients[i].learners_ensemble.learners[0].model)\n",
    "        new_model.eval()\n",
    "        models_test += [new_model]\n",
    "\n",
    "elif setting == 'FedAvg':\n",
    "    \n",
    "#     args_.save_path = 'weights/final/femnist/fig1_take3/fedavg_benign/'\n",
    "#     args_.save_path = 'weights/final/femnist/fig1_take3/FedAvg_adv/'\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    # This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "    hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "    # obtain the state dict for each of the weights \n",
    "    weights_h = []\n",
    "\n",
    "    for h in hypotheses:\n",
    "        weights_h += [h.model.state_dict()]\n",
    "\n",
    "#     weights = np.load(\"weights/final/femnist/fig1_take3/fedavg_benign/train_client_weights.npy\")\n",
    "    weights = np.load(args_.save_path+'train_client_weights.npy')\n",
    "    \n",
    "    # Set model weights\n",
    "    model_weights = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for (w0) in model_weights:\n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            new_weight_dict[key] = w0[0]*weights_h[0][key] \n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        models_test += [new_model]\n",
    "\n",
    "elif setting == 'FedEM':\n",
    "    \n",
    "#     args_.save_path = 'weights/final/femnist/fig1_take3/fedem_benign/'\n",
    "#     args_.save_path = 'weights/final/femnist/fig1_take3/fedem_adv/'\n",
    "#     args_.save_path = 'weights/final/femnist/figperturb/fedem_avg_p0_1/'\n",
    "    aggregator.load_state(args_.save_path)\n",
    "    \n",
    "    # This is where the models are stored -- one for each mixture --> learner.model for nn\n",
    "    hypotheses = aggregator.global_learners_ensemble.learners\n",
    "\n",
    "    # obtain the state dict for each of the weights \n",
    "    weights_h = []\n",
    "\n",
    "    for h in hypotheses:\n",
    "        weights_h += [h.model.state_dict()]\n",
    "\n",
    "#     weights = np.load(\"weights/final/femnist/fig1_take3/fedem_benign/train_client_weights.npy\")\n",
    "#     weights = np.load(\"weights/final/femnist/fig1_take3/fedem_adv/train_client_weights.npy\")\n",
    "    weights = np.load(args_.save_path+\"train_client_weights.npy\")\n",
    "\n",
    "    # Set model weights\n",
    "    model_weights = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        model_weights += [weights[i]]\n",
    "\n",
    "\n",
    "    # Generate the weights to test on as linear combinations of the model_weights\n",
    "    models_test = []\n",
    "\n",
    "    for (w0,w1,w2) in model_weights:\n",
    "        # first make the model with empty weights\n",
    "        new_model = copy.deepcopy(hypotheses[0].model)\n",
    "        new_model.eval()\n",
    "        new_weight_dict = copy.deepcopy(weights_h[0])\n",
    "        for key in weights_h[0]:\n",
    "            new_weight_dict[key] = w0*weights_h[0][key] + w1*weights_h[1][key] + w2*weights_h[2][key]\n",
    "        new_model.load_state_dict(new_weight_dict)\n",
    "        models_test += [new_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261510a",
   "metadata": {},
   "source": [
    "## Set Up Transfer Attack Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eb71a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logs_adv = []\n",
    "\n",
    "for i in range(num_models + 1):\n",
    "    adv_dict = {}\n",
    "    adv_dict['orig_acc_transfers'] = None\n",
    "    adv_dict['orig_similarities'] = None\n",
    "    adv_dict['adv_acc_transfers'] = None\n",
    "    adv_dict['adv_similarities_target'] = None\n",
    "    adv_dict['adv_similarities_untarget'] = None\n",
    "    adv_dict['adv_target'] = None\n",
    "    adv_dict['adv_miss'] = None\n",
    "    adv_dict['metric_alignment'] = None\n",
    "    adv_dict['ib_distance_legit'] = None\n",
    "    adv_dict['ib_distance_adv'] = None\n",
    "\n",
    "    logs_adv += [adv_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce403ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = models_test + [adv_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ed9f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "\t Adv idx: 40\n",
      "120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n",
      "self.y_data.shape torch.Size([120]) batch_size 120\n"
     ]
    }
   ],
   "source": [
    "# Run Measurements for both targetted and untargeted analysis\n",
    "victim_idxs = range(num_models)\n",
    "custom_batch_size = 500\n",
    "adv_idx = num_models\n",
    "\n",
    "for vic_idx in victim_idxs:\n",
    "    print(\"\\t Adv idx:\", adv_idx)\n",
    "    \n",
    "    dataloader = load_client_data(clients = clients, c_id = vic_idx, mode = 'test') # or test/train\n",
    "    \n",
    "    batch_size = min(custom_batch_size, dataloader.y_data.shape[0])\n",
    "    \n",
    "    t1 = Transferer(models_list=all_models, dataloader=dataloader)\n",
    "    t1.generate_victims(victim_idxs)\n",
    "    \n",
    "    # Perform Attacks\n",
    "    t1.atk_params = PGD_Params()\n",
    "    t1.atk_params.set_params(batch_size=batch_size, iteration = 10,\n",
    "                   target = 8, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                   step_size = 0.05, step_norm = \"inf\", eps = 4.5, eps_norm = 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    t1.generate_advNN(adv_idx)\n",
    "    t1.generate_xadv(atk_type = \"pgd\")\n",
    "    t1.send_to_victims(victim_idxs)\n",
    "\n",
    "    # Log Performance\n",
    "    logs_adv[vic_idx]['orig_acc_transfers'] = copy.deepcopy(t1.orig_acc_transfers)\n",
    "    logs_adv[vic_idx]['orig_similarities'] = copy.deepcopy(t1.orig_similarities)\n",
    "    logs_adv[vic_idx]['adv_acc_transfers'] = copy.deepcopy(t1.adv_acc_transfers)\n",
    "    logs_adv[vic_idx]['adv_similarities_target'] = copy.deepcopy(t1.adv_similarities)        \n",
    "    logs_adv[vic_idx]['adv_target'] = copy.deepcopy(t1.adv_target_hit)\n",
    "\n",
    "    # Miss attack\n",
    "    t1.atk_params.set_params(batch_size=batch_size, iteration = 10,\n",
    "                   target = -1, x_val_min = torch.min(data_x), x_val_max = torch.max(data_x),\n",
    "                   step_size = 0.05, step_norm = \"inf\", eps = 4.5, eps_norm = 2)\n",
    "    t1.generate_xadv(atk_type = \"pgd\")\n",
    "    t1.send_to_victims(victim_idxs)\n",
    "    logs_adv[vic_idx]['adv_miss'] = copy.deepcopy(t1.adv_acc_transfers)\n",
    "    logs_adv[vic_idx]['adv_similarities_untarget'] = copy.deepcopy(t1.adv_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d71817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['orig_acc_transfers','orig_similarities','adv_acc_transfers','adv_similarities_target',\n",
    "           'adv_similarities_untarget','adv_target','adv_miss'] #,'metric_alignment']\n",
    "\n",
    "orig_acc = np.zeros([num_models, num_models]) \n",
    "orig_sim = np.zeros([num_models, num_models]) \n",
    "adv_acc = np.zeros([num_models, num_models]) \n",
    "adv_sim_target = np.zeros([num_models, num_models]) \n",
    "adv_sim_untarget = np.zeros([num_models, num_models]) \n",
    "adv_target = np.zeros([num_models, num_models])\n",
    "adv_miss = np.zeros([num_models, num_models]) \n",
    "\n",
    "for adv_idx in range(num_models):\n",
    "    for victim in range(num_models):\n",
    "        orig_acc[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[0]][victim_idxs[victim]].data.tolist()\n",
    "        orig_sim[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[1]][victim_idxs[victim]].data.tolist()\n",
    "        adv_acc[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[2]][victim_idxs[victim]].data.tolist()\n",
    "        adv_sim_target[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[3]][victim_idxs[victim]].data.tolist()\n",
    "        adv_sim_untarget[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[4]][victim_idxs[victim]].data.tolist()\n",
    "        adv_target[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[5]][victim_idxs[victim]].data.tolist()\n",
    "        adv_miss[adv_idx,victim] = logs_adv[victim_idxs[adv_idx]][metrics[6]][victim_idxs[victim]].data.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13521cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_nondiag(array2d):\n",
    "    d1 = array2d.shape[0]\n",
    "    d2 = array2d.shape[1]\n",
    "    \n",
    "    counter = 0\n",
    "    val = 0\n",
    "    \n",
    "    for i1 in range(d1):\n",
    "        for i2 in range(d2):\n",
    "            if i1 != i2:\n",
    "                if not np.isnan(array2d[i1,i2]):\n",
    "                    counter+=1\n",
    "                    val += array2d[i1,i2]\n",
    "    \n",
    "    return val/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c7fa807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adv_target: 0.2247623044066131\n",
      "adv_miss: 0.05582265274432034\n",
      "orig_acc: 0.4718750212341547\n"
     ]
    }
   ],
   "source": [
    "print('adv_target:', avg_nondiag(adv_target))\n",
    "print('adv_miss:', avg_nondiag(adv_miss))\n",
    "print('orig_acc:', np.mean(np.diagonal(orig_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5c99a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_acc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab6c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'adv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d762b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605411df",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'results_celeba/{method}/{setting}_blackbox_orig_acc_transfers', orig_acc)\n",
    "np.save(f'results_celeba/{method}/{setting}_blackbox_orig_similarities', orig_sim)\n",
    "np.save(f'results_celeba/{method}/{setting}_blackbox_adv_acc_transfers', adv_acc)\n",
    "np.save(f'results_celeba/{method}/{setting}_blackbox_adv_similarities_target', adv_sim_target)\n",
    "np.save(f'results_celeba/{method}/{setting}_blackbox_adv_similarities_untarget', adv_sim_untarget)\n",
    "np.save(f'results_celeba/{method}/{setting}_blackbox_adv_target', adv_target)\n",
    "np.save(f'results_celeba/{method}/{setting}_blackbox_adv_miss', adv_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d440cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "celeba",
   "language": "python",
   "name": "celeba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
